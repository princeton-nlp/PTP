# Path
config_name: model_configs/ptp/config.json
tokenizer_name: model_configs/ptp
output_dir: result/my-ptp
run_name: my-ptp
train_file: data/wikibook_256_opt_tk_train.npy
validation_file: data/wikibook_256_opt_tk_val.npy

# Speedup
flash_attn: true
dataloader_num_workers: 8 # Multiple workers to speedup the rendering

# Logging
do_eval: true
do_train: true
eval_steps: 20000
evaluation_strategy: steps
save_steps: 100000
save_strategy: steps
save_total_limit: 15
log_eval_image_pred: true
logging_steps: 10

# Rendering
rendered_as_target: true
replace_new_line: true
add_black_patch: true
add_prefix: true
ignore_white_patches: true

# Image size
font_size: 10
line_space: 6
height: 16
width: 8176 # +CLS to make it 512 sequence length: good for hardware speedup
patch_height: 16
patch_width: 16

# Optimization
num_train_epochs: 16
per_device_eval_batch_size: 32
per_device_train_batch_size: 32
warmup_steps: 50000
lr_scheduler_type: "cosine"
learning_rate: 1.5e-4
min_learning_rate: 1.0e-5
cosine_w_min: true
weight_decay: 0.05
block_size: 256

# Modeling
add_mae_decoder: true
add_text_decoder: true

# Extra
text_mask_rate: 0.25
merge_text_masks: true
mask_ratio: 0.10
span_masking: true
sample_mask_at_collator: true 
norm_pix_loss: true
fp16: true
ignore_mismatched_sizes: true
